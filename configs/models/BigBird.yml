save_dir: save/
model_name: BigBird
# model_weights: abhinavkulkarni/bigbird-roberta-base-finetuned-squad
model_weights: google/bigbird-base-trivia-itc  # TriviaQA
device: cuda

training_parameters:
  lr: 1e-4  # 2e-4
  batch_size: 3
  train_epochs: 10
  warmup_iterations: 1000

# https://huggingface.co/google/bigbird-base-trivia-itc:
# No. of global token = 128
# Window length = 192
# No. of random token = 192
# Max. sequence length = 4096
# No. of heads = 12
# No. of hidden layers = 12
# Hidden layer size = 768
# Batch size = 32
# Loss = cross-entropy noisy spans
