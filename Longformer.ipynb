{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Longformer\n",
    "* arXiv [paper](https://arxiv.org/pdf/2004.05150.pdf)\n",
    "* Huggingface [documentation](https://huggingface.co/docs/transformers/model_doc/longformer)\n",
    "* Medium [blog](https://medium.com/dair-ai/longformer-what-bert-should-have-been-78f4cd595be9#:~:text=The%2030%20layer%20model%2C%20when,(102M%20vs%20277M%20parameters)). Check graphics of memory?\n",
    "\n",
    "\n",
    "config.attention_window\n",
    "global_attention_mask\n",
    "* 0: the token attends “locally”,\n",
    "* 1: the token attends “globally”.\n",
    "\n",
    "\n",
    "Problem of Longformer: Since it's based on window. It might miss elements far from the current token. This might be crucial in tables, for example, where the column name might be many tokens (sequencially ordered) before. Hence, loosing the ability to attend freely to any other token in the document, might 'suponer' loss of information and decrease of performance.\n",
    "Also, it allows up to 4,096 tokens, where we should allow much more...\n",
    "\n",
    "Longformer: \n",
    " * Everything global: Full context\n",
    " * Only questions global: Window of w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it's BERT-like it is an extractive question ansering method.\n",
    "(Comes from RoBERTa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Code in forward:  https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/longformer/modeling_longformer.py#L1964\n",
    "if global_attention_mask is None:\n",
    "    global_attention_mask = _compute_global_attention_mask(input_ids, self.config.sep_token_id)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Up to 4,096 tokens?\n",
    "config.attention_window."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code in forward:  https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/longformer/modeling_longformer.py#L1964\n",
    "if global_attention_mask is None:\n",
    "    global_attention_mask = _compute_global_attention_mask(input_ids, self.config.sep_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to 4,096 tokens?\n",
    "config.attention_window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.config.attention_window)  # len(attention_window) == num_hidden_layers - https://huggingface.co/docs/transformers/model_doc/longformer#transformers.LongformerConfig.attention_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Global attention mask: https://huggingface.co/docs/transformers/v4.18.0/en/model_doc/longformer#transformers.LongformerModel.forward.global_attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First simple test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LongformerTokenizer, LongformerForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4622d675803e43bfb5026a8dcef4198c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=898823.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78cd131497b443c4b78bf2445942b59f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=456318.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6db340b6a26d4e8a87fac6504d6626aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=1355863.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6b99fcf21eb43648f06b0ad0e1a4fcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=866.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4158304ae17946bcb38bf19cebf1c5d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=1738538029.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-large-4096-finetuned-triviaqa\")\n",
    "model = LongformerForQuestionAnswering.from_pretrained(\"allenai/longformer-large-4096-finetuned-triviaqa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n",
    "question, text = [\"Who was Jimmy Hendrinx?\", \"Who was Jim Henson?\"], [\"Jim Henson was a nice puppet\", \"Jim Henson was a nice dog\"]\n",
    "encoding = tokenizer(question, text, return_tensors=\"pt\", padding=True)\n",
    "input_ids = encoding[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default is local attention everywhere\n",
    "# the forward method will automatically set global attention on question tokens\n",
    "attention_mask = encoding[\"attention_mask\"]\n",
    "\n",
    "outputs = model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "puppet\n",
      "dog\n"
     ]
    }
   ],
   "source": [
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "\n",
    "start_idxs = torch.argmax(start_logits, axis=1)\n",
    "end_idxs = torch.argmax(end_logits, axis=1)\n",
    "\n",
    "for bs in range(len(input_ids)):\n",
    "    all_tokens = tokenizer.convert_ids_to_tokens(input_ids[bs].tolist())\n",
    "\n",
    "    answer_tokens = all_tokens[start_idxs[bs] : end_idxs[bs] + 1]\n",
    "    answer = tokenizer.decode(\n",
    "        tokenizer.convert_tokens_to_ids(answer_tokens)\n",
    "    )  \n",
    "\n",
    "    answer = answer.strip() # remove space prepending space token\n",
    "    print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Com tindria sentit que això funcionés:**\n",
    "\n",
    "Que el LongformerForQuestionAnswering fes herència del forward del Longformer on acceptés dos inputs, un per la pregunta i un altre pel context. A la pregunta automàticament posaria global attention = 1 i al context global attention = 2.\n",
    "\n",
    "No obstant a la [línia 1980](https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/longformer/modeling_longformer.py#L1964) no sembla que hi hagi res d'això... Penso que potser el *\\_\\_call()\\_\\_* no crida al *forward()* directament sinó que fa un preprocessament... hauria de posar el codi a Pytorch i debuggar per veure exactament quin és el flow del procés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline DocVQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from transformers import LongformerTokenizer, LongformerForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/SSD/Datasets/DocVQA/Task1/pythia_data\"\n",
    "singledocvqa_dir = os.path.join(root_dir, \"imdb/docvqa/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imdb = np.load(os.path.join(singledocvqa_dir, 'new_imdb_train.npy'), allow_pickle=True)\n",
    "val_imdb = np.load(os.path.join(singledocvqa_dir, 'new_imdb_val.npy'), allow_pickle=True)\n",
    "test_imdb = np.load(os.path.join(singledocvqa_dir, 'new_imdb_test.npy'), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'creation_time': 1637157799.630989,\n",
       " 'version': 1.1,\n",
       " 'dataset_type': 'train',\n",
       " 'has_answer': True}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_imdb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imdb=train_imdb[1:]\n",
    "val_imdb = val_imdb[1:]\n",
    "test_imdb = test_imdb[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-large-4096-finetuned-triviaqa\")\n",
    "model = LongformerForQuestionAnswering.from_pretrained(\"allenai/longformer-large-4096-finetuned-triviaqa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n",
    "question, text = [\"Who was Jimmy Hendrinx?\", \"Who was Jim Henson?\"], [\"Jim Henson was a nice puppet\", \"Jim Henson was a nice dog\"]\n",
    "encoding = tokenizer(question, text, return_tensors=\"pt\", padding=True)\n",
    "input_ids = encoding[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for qa in val_imdb:\n",
    "    question = qa['question']\n",
    "    context = ' '.join([word.lower() for word in qa['ocr_tokens']])\n",
    "    encoding = tokenizer(question, context, return_tensors=\"pt\", padding=True)\n",
    "    input_ids = encoding[\"input_ids\"]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the ‘actual’ value per 1000, during the year 1975?'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}